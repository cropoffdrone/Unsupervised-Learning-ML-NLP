# Unsupervised Learning: Dimensionality Reduction and Clustering

This project focuses on unsupervised learning techniques, specifically dimensionality reduction and clustering, to analyze and understand the structure of the Natural Language Processing dataset.

## Contents

- [Data Exploration and Preprocessing](#data-exploration-and-preprocessing)
- [Dimensionality Reduction](#dimensionality-reduction)
- [Clustering](#clustering)

## Data Exploration and Preprocessing
The data exploration and preprocessing phase involves several key steps:

### 1. Number of samples in train and test dataset
- The first step in the project involves examining the dataset to determine the number of samples in the training and test datasets. This basic statistic provides an overview of the dataset's size and distribution.

### 1.2 Histogram of the length of samples
- A histogram of sample lengths helps us understand the distribution of text length in the dataset, which can be crucial for choosing appropriate preprocessing steps and algorithms.

### 1.3 Sample stats: number of words, chars, symbols, capital letters, etc. in each sample
- This section provides statistics on the characteristics of each sample, including the number of words, characters, symbols, capital letters, and other relevant metrics. These metrics are useful for understanding the structure and complexity of the text data.

### 1.4 Filter special symbols, clean sample's text
- Text preprocessing is essential to clean and prepare the data for further analysis. This step involves removing special symbols, unnecessary characters, and any other noise in the text.

### 1.5 Conversion text to vectors using CountVectorizer or TfIdfVectorizer
- Text data needs to be converted into numerical vectors to apply various machine learning techniques. The use of CountVectorizer or TfIdfVectorizer is common for transforming text into numerical features.

## Dimensionality Reduction
The dimensionality reduction section explores techniques to reduce the data's complexity and visualize it effectively:

### 2.1 Apply PCA to visualize, and explained variance 0.8, 0.9, 0.95 for model training
- Principal Component Analysis (PCA) is a dimensionality reduction technique. In this section, PCA is applied to reduce the dimensionality of the data to visualize it in two dimensions. Different levels of explained variance (e.g., 0.8, 0.9, 0.95) can be considered for model training.

### 2.2 Apply UMAP to compare visualizations
- UMAP (Uniform Manifold Approximation and Projection) is another dimensionality reduction technique. Comparing UMAP visualizations to PCA can help identify patterns and clusters in the data.

## Clustering
The clustering section delves into unsupervised learning methods for identifying patterns within the data:

### 3.1 Perform clustering with K-Means, DBSCAN, and Hierarchical Clustering
- Clustering is an unsupervised learning technique that groups similar data points together. This section explores various clustering algorithms, including K-Means, DBSCAN, and Hierarchical Clustering.

### 3.2 For K-Means, search for the optimal number of clusters with Elbow Method
- K-Means clustering requires specifying the number of clusters (k). The Elbow Method is used to identify the optimal number of clusters by analyzing the within-cluster variance as a function of the number of clusters.

### 3.3 Visualize clusters on two-dimensional samples
- To understand the results of clustering, the clusters are visualized in two dimensions, allowing for a better understanding of the structure within the data.
